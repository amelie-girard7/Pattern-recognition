{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7\n",
    "\n",
    "## Last time\n",
    "\n",
    "- Bias-variance\n",
    "- Classification\n",
    "\n",
    "## Today (Bishop 4.1-4.3, 3.1.3, 4.1.7, 5.2.4, Murphy 8.1-8.3)\n",
    "\n",
    "- Logistic regression MLE\n",
    "- Newton-Raphson\n",
    "- Stochastic gradient descent\n",
    "\n",
    "### Binary classification \n",
    "\n",
    "$y \\in \\{0,1\\}$\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "$Y^{(i)} = 1 \\ | \\ X^{(i)} = x \\overset{indep}{\\sim} Bern [\\sigma(\\theta^T\\phi(x))]$\n",
    "\n",
    "$\\sigma(t) = \\frac{exp(t)}{1 + exp(t)}$\n",
    "\n",
    "(plot sigmoid function)\n",
    "\n",
    "### Optimization\n",
    "\n",
    "Option: Netwon Raphson method\n",
    "- Second order\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1.\n",
    "\n",
    "\n",
    "#### Aside\n",
    "\n",
    "heteroskedasticity\n",
    "\n",
    "### Weighted least squares\n",
    "\n",
    "$\\hat{\\theta}_{MLE} = (\\Phi^TS\\Phi)^{-1}\\Phi^TSy$ for $S=diag(\\sigma_i^2)$\n",
    "\n",
    "Weighted least squares method where we keep iterating is called iterative re-weighted least squares.\n",
    "\n",
    "While logistic regression is a classification model, it also closely tied to these methods of linear regression.\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "Recall: batch vs. streaming\n",
    "\n",
    "Option: stochastic gradient descent\n",
    "\n",
    "- Before: minimizing f($\\theta$)\n",
    "- Now: minimizing $E_{X,Y} \\tilde{f}(\\theta,X,Y)$\n",
    "\n",
    "$$f(\\theta) = \\frac{1}{n} \\log p(y^{(1:n)} \\ | \\ \\theta, x^{(1:n)})$$\n",
    "\n",
    "$$= - \\frac{1}{n} \\Sigma_{i=1}^{n} - \\log (p(y^{(i)} \\ | \\ \\theta, x^{(i)}))$$\n",
    "\n",
    "$$\\approx \\mathbb{E}_{X,Y}\\overset{\\sim}{f} (\\theta,X,Y) \\approx \\frac{1}{n} \\sum_{i=1}^n \\tilde{f}(\\theta, x^{(i)}, y^{(i)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** Find $\\theta^*$ to min $E_{X,Y}\\tilde{f}(\\theta,X,Y)$\n",
    "\n",
    "1. Initialize $\\theta^2$\n",
    "2. For $i = 1,2, ...$\n",
    "\n",
    "$$ (X^{(i)},Y^{(i)}) \\overset{indep}{\\sim} p(x,y) $$\n",
    "$$ \\theta^{( $$\n",
    "\n",
    "Look this up in the book!\n",
    "\n",
    "\"Learning rate schedule\" - look up in book\n",
    "\n",
    "\"Robbins-Monro conditions\" - look up in book\n",
    "\n",
    "**Comparison**\n",
    "\n",
    "IN gradient descent\n",
    "\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} + \\eta_t + \\frac{1}{n} \\nabla \\tilde{f}(\\theta^{(t)}, x^{(i)}, y^{(i)}) $$\n",
    "\n",
    "In gradient descent, every data point appears in our update\n",
    "\n",
    "In stochastic gradient descent, just one data point appears in every update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
