{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11\n",
    "\n",
    "## Last time\n",
    "\n",
    "- Kernels\n",
    "- Kernel trick\n",
    "- Mercer's theorem\n",
    "- Examples: RBF, polynomial\n",
    "\n",
    "## Today\n",
    "\n",
    "- More kernel examples\n",
    "- Kernel ridge regression\n",
    "- When you can kernelize\n",
    "- Neural networks\n",
    "\n",
    "\n",
    "### Kernel examples\n",
    "\n",
    "RBF:\n",
    "\n",
    "$$ k(x,z) = \\exp (-\\frac{1}{2\\sigma^2} || x-z ||_2^2)  $$\n",
    "\n",
    "Polynomial:\n",
    "\n",
    "$$ k(x,z) = (x^T z + 1)^d $$\n",
    "\n",
    "String kernel:\n",
    "\n",
    "$$ x \\in A^d, x' \\in A^{d'} $$\n",
    "\n",
    "Say $s$ is a substring of $x$ if $x = usv$.\n",
    "\n",
    "$u$ and $v$ are some strings, possibly empty.\n",
    "\n",
    "Let $\\phi(x) =$ # of times that $s$ appears in $x$.\n",
    "\n",
    "$$K(x,x') = \\sum_{s \\in A^*} w_s \\phi_s(x) \\phi_s(x') $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples: \n",
    "\n",
    "1. $w_s = 0$ for $|s| > 1$.  Called the \"bag of characters\" kernel.\n",
    "2. $w_s = 0$ unless $s$ has a space at start and end. \"bag of words\" kernel.\n",
    "3. $w_s = 0$ unless $|s| = l$.  $l$-spectrum kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "Let $\\phi_l(x) = (\\phi_s(x))_{s \\in A^l}$\n",
    "\n",
    "$K_l(x,z) = \\phi_l(x) \\phi_l(z)$\n",
    "\n",
    "Create a sorted list of all $l$ substring of $x$ (with the counts), assume that there is some ordering on the alphabet.\n",
    "\n",
    "Create a sorted list of all $l$ substring of $z$ (with the counts), assume that there is some ordering on the alphabet.\n",
    "\n",
    "Go down the list adding up contributions $\\phi_s(x) \\phi_s(z)$.\n",
    "\n",
    "Runtime is dominated by sorting, so runtime is $O(|x|\\log|x|)$\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "As another example, could compare subtrees rather than substrings.\n",
    "\n",
    "\n",
    "### Ridge regression with kernels\n",
    "\n",
    "Remember ridge regression:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^n(\\theta^T \\phi(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2} ||\\theta||^2$$\n",
    "\n",
    "Prof. Bresler claims it's hard to see how we can kernelize this.\n",
    "\n",
    "Set $$\\nabla_{\\theta} J(\\theta) = 0 \\Rightarrow \\Phi^T \\Phi \\theta + \\lambda \\theta = \\Phi^T Y$$\n",
    "\n",
    "$$ \\theta = (\\Phi^T\\Phi + \\lambda I_d)^{-1} \\Phi^T Y $$\n",
    "\n",
    "Rewrite (1):\n",
    "\n",
    "**Matrix Inversion Lemma** (on page 117 of Murphy)\n",
    "\n",
    "$$M = \\begin{bmatrix}E & F \\\\ G & H \\end{bmatrix}, E, H \\text{invertible}$$\n",
    "\n",
    "...look up in Murphy\n",
    "\n",
    "\n",
    "Set\n",
    "\n",
    "$E = \\lambda I_d$\n",
    "\n",
    "$F = -\\Phi^T$\n",
    "\n",
    "$G = \\Phi$\n",
    "\n",
    "$H = I_n$\n",
    "\n",
    "Use matrix inversion lemma and get\n",
    "\n",
    "$$ \\theta = \\Phi^T(\\Phi \\Phi^T + \\lambda I_n)^{-1} Y $$\n",
    "\n",
    "$$ K(x^{(i)},x^{(j)} = <\\phi(x^{(i)}, \\phi(x^{(j)}> $$\n",
    "\n",
    "$ K = \\Phi \\Phi^T $ gram matrix\n",
    "\n",
    "$$ \\alpha = (K + \\lambda I_n)^{-1} Y $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ \\theta = \\Phi^T\\alpha = \\sum_{i=1}^n \\alpha_i \\phi(x^{(i)}) $$\n",
    "\n",
    "$$ h(x) = \\theta^T \\phi(x) = \\sum_{i=1}^n \\alpha_i k(x^{(i)}, x) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when can I kernelize?  What else can I kernelize?\n",
    "\n",
    "Let's consider a problem where we have a predictor $h(x) = g(<\\theta, \\phi(x)>$\n",
    "\n",
    "$$\\underset{\\theta}{\\text{min}} \\{f(<\\theta, \\phi(x^{(i)}>, ..., <\\theta, \\phi(x^{(n)})>) + R(||\\theta||) \\ \\ \\ (2)$$\n",
    "\n",
    "$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ arbitrary\n",
    "\n",
    "$R: \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ non-decreasing\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "Soft-SVM\n",
    "\n",
    "$R(a) = \\frac{\\lambda}{2} a^2$\n",
    "\n",
    "$f(a_1,...,a_n) = \\frac{1}{n} \\sum_i max \\{0, 1 - y^{(i)} a_i\\}$\n",
    "\n",
    "Hard-SVM\n",
    "\n",
    "$R(a) = a^2$\n",
    "\n",
    "$f(a_1, ..., a_n) = \\{0 \\text{ if } \\exists \\ b \\ s.t. y^{(i)}(a_i + b) \\ge 1, \\infty otherwise \\}$\n",
    "\n",
    "**Theorem** Representer Theorem\n",
    "\n",
    "Assume that $\\phi$ is a mapping from $X$ to a Hilbert Space.  Then, there exists a vector $\\alpha \\in \\mathbb{R}^n$ s.t. $\\theta = \\sum_{i=1}^n \\alpha_i \\phi(x^{(i)})$ that's an optimal solution of (2)\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Let $\\theta$^* be an optimal solution to (2)\n",
    "\n",
    "$\\theta^* = \\sum_{i=1}^n a_i \\phi(x^{(i)}) + u$   $ \\ \\ \\ <u, \\phi(x^{(i)})> = 0 \\ \\ \\ \\forall i$\n",
    "\n",
    "$\\theta = \\theta^* - u$\n",
    "\n",
    "Pythagoras says $||\\theta^*||^2 = ||\\theta||^2 + ||u||^2$\n",
    "\n",
    "$\\Rightarrow ||\\theta|| \\le ||\\theta^*||$\n",
    "\n",
    "$\\Rightarrow R(||\\theta||) \\le R(||\\theta^*||)$\n",
    "\n",
    "$$ <\\theta, \\phi(x^{(i)}> = <\\theta^* - u, \\phi(x^{(i)})> = <\\theta^*, \\phi(x^{(i)})> $$\n",
    "\n",
    "$f(\\theta) = f(\\theta^*)$\n",
    "\n",
    "$\\theta = \\ sum_{i=1}^n \\alpha_i \\phi(x^{(i)})$\n",
    "\n",
    "$||\\theta||^2 = \\sum_{i,j} \\alpha_i \\alpha_j K(x^{(I)}, x^{(j)}$\n",
    "\n",
    "Prediction $h(x) = g(M\\theta, \\phi(x)>) = g(\\sum_{i=1}^n \\alpha_i K(x^{(i)}, x))$\n",
    "\n",
    "\n",
    "## Linear and logistic regression, united\n",
    "\n",
    "Can write down in one general form, both linear and logistic regression\n",
    "\n",
    "$\\mathbb{E} [ Y \\ | X = x, \\theta] = g(\\sum_{j=1}^d \\theta_j \\phi_j(x))$\n",
    "\n",
    "For logistic regression, $g(t) = \\frac{1}{1 + e^{-t}}$\n",
    "\n",
    "For linear regression, $g(t) = t$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning basis functions from data\n",
    "\n",
    "Neural networks work well for this, so do other methods.\n",
    "\n",
    "$$ a_j^{(1)} = \\sum_{i=1}^M w_{ji}^{(1)}x_i + w_{j0}^{(1)} $$\n",
    "\n",
    "$$ z_j = g(a_j) $$\n",
    "\n",
    "In general $g$ will be a nonlinear function.  For example could be the logistic function, hyperbolic tangent, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ inputs \\ \\ \\ x_1, ..., x_d$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universal Approximation**\n",
    "\n",
    "Look up this theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
