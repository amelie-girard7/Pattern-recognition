{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "\n",
    "## Last time\n",
    "- Maximum a Posteriori estimation\n",
    "- MAP of Gaussian regression is ridge regression\n",
    "- BIas-variance tradeoff for prediction\n",
    "- LASSO / other regularizers\n",
    "\n",
    "\n",
    "## Today (Bishop 1.5, 3.2, Murphy 6.4, Bishop 4, 4.3)\n",
    "\n",
    "- Bias-variance trade-off for estimation\n",
    "- Classification\n",
    "\n",
    "\n",
    "### Prediction vs. estimation\n",
    "\n",
    "Recall: Loss $L(guess,actual)$\n",
    "\n",
    "Estimation: \n",
    "- \"actual\" is a parameter $\\theta$ of the model\n",
    "- \"guess\" is a function $\\hat{\\theta}(x^{(1:n)}, y^{(1:n)})$\n",
    "\n",
    "Prediction:\n",
    "- \"actual\" is now a new data point\n",
    "- \"guess\" is a function $h(D, x^{(n+1)})$\n",
    "\n",
    "###Risk in Estimation\n",
    "\n",
    "Risk = $E_D L(\\hat{\\theta}, \\theta)$\n",
    "\n",
    "###Squared error loss\n",
    "\n",
    "Let's think about squared error loss in relation to estimation\n",
    "\n",
    "$L(\\hat{\\theta}, \\theta) = (\\hat{\\theta} - \\theta)^2$\n",
    "\n",
    "$E_d(\\hat{\\theta} - \\theta) = E_D(\\hat{\\theta} - E_D\\hat{\\theta} + E_D\\hat{\\theta} - \\theta)^2$\n",
    "\n",
    "$=E_D(\\hat{\\theta} - E_D\\hat{\\theta})^2 + (E_D\\hat{\\theta} - \\theta)^2$\n",
    "\n",
    "First term above is variance, second term above is bias^2\n",
    "\n",
    "Bias and variance are the same as accuracy and precision, conceptually\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "$Y^{(i)} \\sim N(\\mu, \\sigma^2)$\n",
    "\n",
    "Want to estimate $\\mu$\n",
    "\n",
    "$\\hat{\\mu}_1 = Y^{(1)}$\n",
    "\n",
    "or\n",
    "\n",
    "$\\hat{\\mu}_N = \\frac{1}{n} \\sum_{i=1}^n Y^{(i)}$\n",
    "\n",
    "#### Example\n",
    "\n",
    "$Y^{(i)} \\sim N(\\mu, \\sigma^2)$\n",
    "\n",
    "$\\sigma_a^@ = \\frac{1}{a}\\sum_{i=1}^n(Y^{(i)} - \\frac{1}{n} \\sum_{j=1}^n Y^{(j)})^2$\n",
    "\n",
    "$a = n \\Rightarrow MLE$\n",
    "\n",
    "Hey, MLE is biased, why don't we checked out unbiased estimator\n",
    "\n",
    "$a = n-1 \\Rightarrow$ an unbiased estimator for $\\sigma^2$\n",
    "\n",
    "(Check out Wikipedia article on mean square error)\n",
    "\n",
    "$a = n+1$ so if you really care about squared error loss\n",
    "\n",
    "### Shrinkage\n",
    "\n",
    "Averaging an estimator with the origin with $0_d$ (or some other fixed point) tends to improve its risk for squared error loss.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Ridge regression, LASSO, etc.  $\\underset{\\theta}{\\text{min}} \\text{ RSS}(\\theta) + \\lambda || \\theta ||_q$\n",
    "\n",
    "#### Example (bonus)\n",
    "\n",
    "Stein's phenomenon.\n",
    "\n",
    "Take one data point, $Y^{(1)}$\n",
    "\n",
    "$Y^{(1)} \\sim N(\\mu, \\sigma^2I_{d \\times d}), d > 2$\n",
    "\n",
    "$\\hat{\\mu}_{MLE} = Y^{(1)}$  Stigler 1990 paper\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "Risk = $E_{X^{(n+1)},Y^{(n+1)}} [ (h(D, X^{(n+1)}) - Y^{(n+1)})^2]$\n",
    "\n",
    "$= E_{X^{(n+1)}} [(h(D,X^{(n+1)}) - E[Y^{(n+1)} \\ | \\ X^{(n+1)}])^2] + E_{X^{(n+1)}} \\text{ Var} (Y^{(n+1)} \\ | \\ X^{(n+1)})$\n",
    "\n",
    "In general for prediction, we have some true curve.  For each x, we have some mean.  Around that t(x), we generate some data.  There is some intrinsic noise of the data around the curve.\n",
    "\n",
    "The thing that says variance is actually the intrinsic noise.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Ridge regression.\n",
    "\n",
    "Imagine that we actually know the truth, and it's the green line.\n",
    "\n",
    "$E[Y \\ | \\ X]:$ green line\n",
    "\n",
    "$\\phi$: 25 basis functions\n",
    "\n",
    "100 data sets\n",
    "\n",
    "25 data points per set\n",
    "\n",
    "## Classification\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "Time $x^{(i)}$ is time that student studied for exam\n",
    "\n",
    "$y^{(i)}$ is whether i passed $1 = pass, 0 = not pass$\n",
    "\n",
    "We can just use linear regression, but that doesn't work well\n",
    "\n",
    "Recall: $y^{(i)} \\overset{indep}{\\sim} N(\\theta^T \\phi(x^{(i)}), \\sigma^2)$\n",
    "\n",
    "MLE $\\Rightarrow$ OLS\n",
    "\n",
    "MAP $\\Rightarrow$ ridge regression\n",
    "\n",
    "Model misspecification: The \"true\" mechanism generating the data isn't in the model family\n",
    "\n",
    "Observe data $(x^{(i)}, y^{(i)})$ for $i = 1,...,n$\n",
    "\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$\n",
    "- $y^{(i)} \\in \\{ C_1, ..., C_K\\}$\n",
    "\n",
    "Task is to come up with some prediction $\\hat{y} = h(x)$\n",
    "\n",
    "Risk: $E_{XY} L(h(X), Y)$\n",
    "\n",
    "Gaussian noise is really intrinsically tied with square error loss\n",
    "\n",
    "###0-1 loss\n",
    "\n",
    "$L(h(x),y) = 1 \\{h(x) \\ne y\\}$\n",
    "\n",
    "### Consider K=2\n",
    "\n",
    "$P(Y^{(i)} = C_1 \\ | \\ X^{(i)} = x) = sigmoid(\\theta^T x)$\n",
    "\n",
    "#### Example\n",
    "\n",
    "$\\phi(x) = (x_1, x_2)^T$    Murphy, uses w for $\\theta$\n",
    "\n",
    "### Prediction \n",
    "\n",
    "Assume $\\theta$ fixed and known\n",
    "\n",
    "As always, we go to the risk.  What is going to be the prediction that minimizes the risk?\n",
    "\n",
    "Risk $= E_{X,Y}1\\{h(X) \\ne Y\\}$\n",
    "\n",
    "$= P_{X,Y} (h(X) \\ne Y)$\n",
    "\n",
    "$h^* = \\{C_1 if \\ q(x) > 0.5, C_2 if \\ q(x) < 0.5\\}$\n",
    "\n",
    "For each point, we want to predict the highest probability class.\n",
    "\n",
    "At a high conceptual level, we are at the same point we were for regression.\n",
    "\n",
    "The problem is that our model implicitly depends on some parameter, $\\theta$.\n",
    "\n",
    "Recall that with regression, we could use $\\hat{\\theta}_{\\text{MLE}$ or $\\hat{\\theta}_{\\text{MAP}$ or take a fully Bayesian approach and have a prior on $\\theta$, then calculate posterior on $\\theta$ and predictive on $\\theta$\n",
    "\n",
    "With regression, everything was Gaussian.  Noise was Gaussian... priors were Gaussian and led to Gaussian posteriors.  Nicely got closed-form solution for MLE and MAP.  \n",
    "\n",
    "Classification is much more realistic in that there are no closed form solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
