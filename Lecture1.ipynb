{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1\n",
    "\n",
    "## 6.867 Introduction\n",
    "\n",
    "- Recitation assignments on *Stellar*\n",
    "- All homework, announcements, etc on *Piazza*\n",
    "\n",
    "Instructors\n",
    "- Guy Bresler\n",
    "- Tamara Broderick\n",
    "- Leslie Kaelbling\n",
    "\n",
    "An ML algorithm is a program that learns form experience\n",
    "\n",
    "Course objectives:\n",
    "\n",
    "i) Apply cutting-edge ML methods\n",
    "ii) Modify existing algorithms and develop new ones\n",
    "iii) Understand why do certain algorithsm work well in certain settings\n",
    "\n",
    "\n",
    "## Task, Training Data (Examples), Performance Metric\n",
    "\n",
    "Example: Spam detection (classification)\n",
    "\n",
    "Task: design program to decide if message is spam or not\n",
    "\n",
    "Training data: examples of SPAM and not SPAM messages\n",
    "\n",
    "Performance Metric: minimize $P(h(x) \\ne y)$\n",
    "\n",
    "\n",
    "Some reasonable rules:\n",
    "\n",
    "h(x) = SPAM if {unknown sender} AND {subject contains 'meds' or '$'}\n",
    "\n",
    "h(x) = SPAM if 2 $\\times$ 1{money} + 3 $\\times$ 1{meds} + O.S.(# spelling errors)$^2$\n",
    "\n",
    "h(x) = SPAM if time sent(x) $\\in$ timesent$(x)^{(i)}$ such that $y^{(i)}$ = 1}\n",
    "\n",
    "\n",
    "**Supervised Learning**\n",
    "\n",
    "$(x^{(i)}, y^{(i)}, ..., x^{(n)}, y^{(n)})$ labeled training data\n",
    "\n",
    "$x^{i} \\in \\mathbb{X}$, $y^{(i)} \\in \\mathbb{Y}$\n",
    "\n",
    "$(x^{(i)}, y^({i)}) ~ p(x,y)$ learning algorithm\n",
    "\n",
    "x --> hypothesis --> y\n",
    "\n",
    "\n",
    "*Example*\n",
    "\n",
    "Task: observe a 2 second video of soccer play and predict if results in goal\n",
    "\n",
    "Examples: many clips labeled 0 or 1\n",
    "\n",
    "Performance metric: maximize expected\n",
    "\n",
    "\n",
    "\n",
    "*Example*\n",
    "\n",
    "Task: predict the stopping distance of a car based on mass, speed\n",
    "\n",
    "Examples: lots of data from stopping over and over again\n",
    "\n",
    "Performance metric: might want to have a cost function that really heavily  overestimates the stopping distance!\n",
    "\n",
    "\n",
    "\n",
    "If $\\mathbb{y}$ is finite: \"classification\"\n",
    "\n",
    "If $\\mathbb{y}$ is continuous: \"regression\"\n",
    "\n",
    "Side note: discrete and infinite is classification\n",
    "\n",
    "\n",
    "**Loss function**\n",
    "\n",
    "L: $Y \\times Y \\rightarrow \\mathbb{R}$\n",
    "\n",
    "0-1 Loss: L(h,y) = {0 if h = y, 1 otherwise}\n",
    "\n",
    "General loss function: L(h,y) = $sum_j \\sum_k L_{jk} \\mathbb{1} {h = j, y = k}\n",
    "\n",
    "Squared loss: $Y \\subset \\mathbb{R}$\n",
    "\n",
    "$L(h,y) = (h-y)^2$\n",
    "\n",
    "Linear loss: $L(h,y) = | h-y |$\n",
    "\n",
    "\n",
    "Could also minimize \"Risk\": the average value of the y's\n",
    "\n",
    "$\\mathbb{E}[L] = \\mathbb{P}[h(x) \\ne y]$\n",
    "\n",
    "\n",
    "Suppose we know $p(x,y)$\n",
    "\n",
    "Claim: For general loss matrix $L_i$\n",
    "\n",
    "$h(x) = \\underset{x}{argmin} \\sum_j L_jk p(y=j \\ | \\ x)$\n",
    "\n",
    "Proof: Risk = \\mathbb{E} [ something can't read ] $= \\sum_k \\int \\sum_j L_{jk} p(x,y=j) dx$\n",
    "\n",
    "If we have the distribution for x and y, then designing the optimal hypothesis is easy and is given by this formula.\n",
    "\n",
    "\n",
    "Regression with squared loss\n",
    "\n",
    "$h* = \\underset{h}{argmin} \\mathbb{E}[(h(x)-y)^2]$\n",
    "\n",
    "Claim but won't go through derivation $h*(x) = \\mathbb{E}_y[y|x] = \\int_y y \\ p(y|x)dy$\n",
    "\n",
    "Proof: pg 46-47 of Bishop\n",
    "\n",
    "\n",
    "If had p(x,y) then we are happy.\n",
    "\n",
    "\n",
    "Another idea which bypasses first trying to estimate the joint distribution.\n",
    "\n",
    "Let's alternatively try to estimate the risk $\\mathbb{E}[L(h(x),y)]$ directly?\n",
    "\n",
    "? $\\approx$ \\frac{1}{N} \\sum_{i=1}^N L(h(x^{(i)},y^{(i)})$\n",
    "\n",
    "This above is the empirical risk.\n",
    "\n",
    "\n",
    "Law of large numbers:\n",
    "\n",
    "Let $Z_1, ..., Z_N$ iid ~ $p(z)$ s.t. $\\mathbb{E}[Z_1] < \\infty$\n",
    "\n",
    "The tilde means distributed according the p(z)\n",
    "\n",
    "Then $\\frac{1}{N} \\sum_{i=1}^N Z_i \\rightarrow \\mathbb{E}[Z_i]$\n",
    "\n",
    "\n",
    "Choosing $h \\in \\mathbb{H}$ minimizing empirical risk is called ERM (Empirical Risk Minimization)\n",
    "\n",
    "So, choosing $h \\in \\mathbb{H}$ is important!\n",
    "\n",
    "\n",
    "Back to estimating p(x,y)\n",
    "\n",
    "Often done by fitting a *parametric model* to the data\n",
    "\n",
    "Data: $y^{(i)}, ..., y^{(n)} \\in {0,1}$.  y^{(i)} ~ Ber(\\theta)  \\mathbb{P}(y^{(i)}=1) = \\theta$\n",
    "\n",
    "Goal: find best parameter\n",
    "\n",
    "\n",
    "Choose $\\theta*$ to maximize some score\n",
    "\n",
    "**Maximum Likelihood**: score($\\theta$) $= \\mathbb{P}(D \\ | \\ \\theta) = \\prod_{i=1}^N \\theta^y(i)(1-\\theta)^{1 - y(i)}$\n",
    "\n",
    "\n",
    "Other machine learning problems you might be interested in.\n",
    "\n",
    "Might be thinking \"oh but mine isn't a supervised classification problem\".\n",
    "\n",
    "Semi-supervised learning.\n",
    "\n",
    "Unsupervised learning -- don't have labels for your training data.  Might want to estimate probability distribution for the data, might want to \n",
    "\n",
    "Reinforcement learning -- very interesting and quite different.  You have a robot going around and trying things out.  Has sensors, gets some feedback, it's training data depends on its actions.  Guy Breseler's interest in reinforcement learning is so that he can find the best way to sleep.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
