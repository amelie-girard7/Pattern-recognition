{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4\n",
    "## September 22, 2015\n",
    "\n",
    "### Last time (Bishop 3.1)\n",
    "\n",
    "- Regression\n",
    "- Ordinary least squares (OLS) = $\\underset{\\theta}{\\text{argmin}} \\sum_{i=1}^n (y^{(i)} - \\theta^Tx^{(i)})^2$\n",
    "Features are $x^{(i)}$, observations of them are $y^{(i)}$\n",
    "- Conditional Gaussian MLE $\\sim$\n",
    "\n",
    "### Today\n",
    "\n",
    "- Bayes + Gaussians\n",
    "- Bayesian regression\n",
    "- Ridge regression\n",
    "- (Bias-variance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we're going to consider a Bayes approach to maximum likelihood.\n",
    "\n",
    "We'll build up to talking about Bayesian regression.\n",
    "\n",
    "Then we'll see how this magically becomes ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall Bayes Thm\n",
    "\n",
    "$$ p(\\theta \\ | y \\ ) \\propto_\\theta p(y \\ | \\ \\theta) p(\\theta) $$\n",
    "\n",
    "Recall conjugate priors\n",
    "\n",
    "Consider multivariate Gaussian likelihood [HW \\#0, Bishop 2.3]\n",
    "\n",
    "$$ Y^{(i)} \\overset{i.i.d.}{\\sim} N(\\mu, \\Sigma) $$\n",
    "\n",
    "Data, $Y^{(i)}$ is a column of dimension d.\n",
    "\n",
    "Mean, $\\mu$, is a column of dimension d.\n",
    "\n",
    "Covariance matrix, $\\Sigma$ is a $d \\times d$ matrix, and is positive definite\n",
    "\n",
    "$$p(y^{(i)} \\ | \\ \\mu)$$ = multivariate gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could go through all the math, and that's correct, just takes a while.  Could also just say, hey this has a quadratic form, so we need to have a prior with a quadratic form.\n",
    "\n",
    "Here we have the posterior as a normal distribution:\n",
    "\n",
    "$$ p(\\mu \\ | y^{(i)}) = N(\\mu \\ | \\ \\mu_1, \\Sigma_1) $$\n",
    "\n",
    "$$ \\propto \\exp \\{ -  \\frac{1}{2}(\\mu - \\mu_1)^T \\Sigma_1^{-1} (\\mu - \\mu_1) \\} $$\n",
    "\n",
    "$$ \\mu^T \\Sigma^{-1}\\mu + \\mu^T \\Sigma_0^{-1}\\mu = \\mu^T\\Sigma_1^{-1}\\mu $$\n",
    "\n",
    "$$ \\Sigma_1^{-1} = \\Sigma^{-1} + \\Sigma_0^{-1}  \\ \\ \\ \\ \\ \\text{this is precision matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y^{{(1)}^T} \\Sigma^{-1}\\mu + \\mu_0^T \\Sigma_0^{-1}\\mu = \\mu_1^T \\Sigma_1^{-1}\\mu$$\n",
    "\n",
    "$$ \\Sigma_1^{-1} \\mu_1 = \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}y^{(1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian sequential learning**\n",
    "\n",
    "batch: all data at once\n",
    "\n",
    "streaming: some data at time intervals\n",
    "\n",
    "$$ Y^{(i)} \\overset{iid}{\\sim} p(y \\ | \\ \\theta)$$\n",
    "\n",
    "$$ p(\\theta \\ | \\ y^{(1 : n)} \\propto_\\theta p(y^{(1:n)} \\ | \\ \\theta) p(\\theta)$$\n",
    "\n",
    "$$ = \\big[ \\prod_{i=1}^n p(y^{(i)} \\ | \\ \\theta \\big] p(\\theta) $$\n",
    "\n",
    "$$ = p(y^{(1:m)} \\ | \\ \\theta) p(y^{(m+1:n)} \\ | \\ \\theta) p(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\propto p( \\theta \\ | \\ y^{(1:m)}) p(y^{(m+1:n)} \\ | \\ \\theta)  $$\n",
    "\n",
    "First term above is the old posterior, which we then use as the new prior\n",
    "\n",
    "Point is that you get the same thing whether you take a streaming or batch approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\mu \\ | \\ y^{(1:n)} = N(\\mu \\ | \\ \\mu, \\Sigma_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Sigma_n^{-1}= n\\Sigma^{-1} + \\Sigma_0^{-1} $$\n",
    "\n",
    "$$ \\Sigma_n^{-1} \\mu_n = \\Sigma_0^{-1} \\mu_0 + \\Sigma^{-1} (\\Sigma_{i=1}^n y^{(i)} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing we'll do is apply this to regression.  What we've done at this point is talk about a bunch of data that are i.i.d. from a Gaussian distribution.\n",
    "\n",
    "But regression isn't so different.  It's pretty close.  We're just muliplying the mean by some linear term.\n",
    "\n",
    "**Back to regression**\n",
    "\n",
    "$$ Y^{(i)} \\overset{indep}{\\sim} N(\\theta^T \\phi(x^{(i)}), \\sigma^2)$$\n",
    "\n",
    "Assume $ \\sigma^2 > 0 $ is fixed and known\n",
    "\n",
    "Assume $ \\theta \\in \\mathbb{R}^d $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case again have Gaussian likelihood\n",
    "\n",
    "$$ p(y^{(i)} \\ | \\ \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} ( y^{(i)} - \\theta^T \\phi(x^{(i)}))^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\theta) = N(\\theta \\ | \\ \\mu_0, \\Sigma_0)$$\n",
    "\n",
    "$$ p(\\theta \\ | \\ y^{(i)})   = N(\\theta \\ | \\ \\mu_1, \\Sigma_1)$$\n",
    "\n",
    "$$ \\Sigma_1^{-1} = \\Sigma_0^{-1} + (\\sigma^2)^{-1} \\phi(x^{(1)}) \\phi(x^{(1)})^T$$\n",
    "\n",
    "In fact if we look at the linear terms then we get the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Sigma_1^{-1} \\mu_1 = \\Sigma_0^{-1} \\mu_0 + (\\sigma^2)^{-1} \\phi(x^{(1)})y^{(1)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
