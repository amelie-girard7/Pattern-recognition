{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 8\n",
    "\n",
    "## Last time\n",
    "\n",
    "- Logistic regression\n",
    "- Maximum likelihood\n",
    "- Newton-Raphson\n",
    "- Stochastic Gradient Descent\n",
    "\n",
    "## Today\n",
    "\n",
    "- More on logistic regression... wrap up SGD\n",
    "- Support Vector Machine, SGD for SVM\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "- Before $y \\in \\{0, 1\\}$\n",
    "- Today $y \\in \\{-1.+1\\}$\n",
    "\n",
    "$$P(Y = 1 \\ | \\ \\theta, X = x) = \\sigma(\\theta^T \\phi(x))$$\n",
    "\n",
    "$$P(Y = -1 \\ | \\ \\theta, X = x) = 1 - P(Y = 1 \\ | \\ \\theta, X = x)  = 1 - \\sigma(\\theta^T \\phi(x)) = \\sigma(-\\theta^T \\phi(x))$$\n",
    "\n",
    "$$p(y \\ | \\ x) = \\sigma(y\\theta^T\\phi(x))$$\n",
    "\n",
    "\n",
    "MLE\n",
    "\n",
    "Minimize $-\\log p(y^{(1:n)} \\ | \\ \\theta, x^{(1:n)}) = \\Sigma_{i=1}^n - \\log p(y^{(i)} \\ | \\ \\theta_1 x^{(i)}) $\n",
    "\n",
    "$= \\sum_{i=1}^n - \\log \\sigma(y^{(i)} \\theta^T \\phi(x^{(i)})) $\n",
    "\n",
    "$= \\sum_{i=1}^n \\log [ 1 + \\exp(-y^{(i)}\\theta^T\\phi(x^{(i)}))] $\n",
    "\n",
    "Function to the right of sum sign is $\\tilde{f}(\\theta, x^{(i)}, y^{(i)})$\n",
    "\n",
    "$\\nabla_{\\theta} \\sum_{i=1}^n \\tilde{f}(\\theta, x^{(i)}, y^{(i)}) = ...$\n",
    "\n",
    "We don't always have the ability to store the whole dataset, so stochastic gradient descent makes a lot of sense.\n",
    "\n",
    "SGD:\n",
    "\n",
    "1. Start with some $\\theta^{(0)}$\n",
    "2. for i = 1,2,...\n",
    " 1. $(X^{(i)}, Y^{(I)}) \\overset{iid}{\\sim} p(x,y)$\n",
    " 2. $\\theta^{(i+1)} = \\theta^{(i)} + \\eta_i Y^{(i)} \\phi(x^{(i)}) [1 - \\sigma(Y...$\n",
    " \n",
    "Papers on SGD, Robbins & Monro 1951.  Also Nemirovski and Yudin 1978\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "$P(Y=1 \\ | \\ X=x) = \\sigma(\\theta^T\\phi(x))$\n",
    "\n",
    "$h(x) = sign(\\sigma(\\theta^T\\phi(x)) - \\frac{1}{2})$\n",
    "\n",
    "\n",
    "\n",
    "0-1 Loss $L(h,y) = \\mathbb{1}\\{y \\ne h\\} = \\mathbb{1}\\{yh<0\\}$\n",
    "\n",
    "Risk = $P(h(x) \\ne y)$\n",
    "\n",
    "ERM: $\\underset{\\theta}{minimize} \\sum_{i=1}^n \\mathbb{1} \\{ y^{(i)} \\theta^T \\sigma(x^{(i)}) < 0\\}$\n",
    "\n",
    "\n",
    "Might be hard to minimize the 0-1 loss.  So we can use one of the 0-1 surrogates:\n",
    "\n",
    "\n",
    "0-1: $l(z) = \\mathbb{1}(z<0)$\n",
    "\n",
    "logistic: $l(z) = \\log [ 1 + exp(-z)]$\n",
    "\n",
    "hinge: $l(z) = max\\{0,1-z\\}$\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Hard SVM:\n",
    "\n",
    "Input $(x^{(i)},y^{(i)})_{i=1}^n$\n",
    "\n",
    "$\\underset{\\theta,\\theta_0}{Maximize} \\frac{1}{||\\theta||} \\underset{i}{min} [ y^{(i)} (\\theta^T x^{(i)} + \\theta_0) ]$\n",
    "\n",
    "\n",
    "Can scale $\\theta$ by whatever you want.\n",
    "\n",
    "Fix $||\\theta|| = 1$, produces a sphere, don't know how to optimize over that since it's not convex\n",
    "\n",
    "Scale $\\theta$ so that $\\underset{i}{min}[y^{(i)} (\\theta^T x^{(i)} + \\theta_0)] = 1$\n",
    "\n",
    "$y^{(i)} (\\theta^T x^{(i)} + \\theta_0) \\ge 1 $\n",
    "\n",
    "**Hard SVM**\n",
    "\n",
    "Minimize $\\frac{1}{2}||\\theta||^2$ such that $y^{(i)}(\\theta^T x^{(i)} + \\theta_0) \\ge 1)$\n",
    "\n",
    "This is a QP, has a quadratic objective and affine constraints.  Can solve on a computer.  Yay!\n",
    "\n",
    "\n",
    "What is this max margin thing doing?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Soft-SVM** (Cortes-Vapnik '93-95)\n",
    "\n",
    "$Y_i (\\theta^T x^{(i)} + \\theta_0) \\ge 1 \\rightarrow y_i(\\theta^T x^{(i)} + \\theta_0 \\ge 1 - \\psi_i$ , $\\psi_i \\ge 0, \\forall i$\n",
    "\n",
    "Soft-SVM: Input $(X^{(i)},Y^{(i)})_{i=1}^n, \\lambda > 0$\n",
    "\n",
    "$$ \\underset{\\theta,\\theta_0}{minimize} \\frac{\\lambda}{2} ||\\theta||^2 + \\frac{1}{n}\\sum_{i=1}^n \\psi_i$$\n",
    "\n",
    "Equation above is same as\n",
    "\n",
    "$$ \\underset{\\theta,\\theta_0}{min} \\bigg[  \\frac{\\lambda}{2}||\\theta||^2 + \\frac{1}{n}\\sum l^{hinge}(y^{(i)}(\\theta^T\\phi(x)) \\bigg] $$\n",
    "\n",
    "\n",
    "Proof: fix $(\\theta, \\theta_0)$, consider maximizer in $\\psi$\n",
    "\n",
    "For each $i: \\psi_i = \\{ 0 if y^{(i)}(\\theta^Tx + \\theta_0) \\ge 1,   \\ \\ \\ \\ or 1 - y^{(i)}(\\theta^Tx + \\theta_0) otherwise$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SGD for SVM\n",
    "\n",
    "One approach to computing SVM really fast is SGD.\n",
    "\n",
    "$$\\nabla_{\\theta} ||\\theta||^2 \\frac{\\lambda}{2} = \\lambda \\theta $$\n",
    "\n",
    "We can't differentiate the hinge function at the elbow point.  What do we do?  We can compute a sub-gradient.\n",
    "\n",
    "*Subgradient*\n",
    "\n",
    "For a smooth convex $f$\n",
    "\n",
    "Can define the gradient at $\\theta$ as the unique $v$ s.t. $\\forall w, f(w) \\ge f(\\theta) + (w-\\theta,v)$\n",
    "\n",
    "For a non-smooth function, call $v$ a subgradient.  $v \\in \\partial f(\\theta)$ set.  subdifferential\n",
    "\n",
    "**Claim:** Let $g(\\theta) = \\underset{i \\in [r]}{max} g_i(\\theta)$, where $g_i$ convex $\\forall i$ and smooth\n",
    "\n",
    "Given some $\\theta$, let $j \\in \\underset{i}{argmax} g_i (\\theta)$ then $\\nabla g_j(\\theta) \\in \\partial g(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
